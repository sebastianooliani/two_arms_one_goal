@ARTICLE{10322775,
  author={Tika, Argtim and Bajcinca, Naim},
  journal={IEEE Transactions on Control Systems Technology}, 
  title={Predictive Control of Cooperative Robots Sharing Common Workspace}, 
  year={2024},
  volume={32},
  number={2},
  pages={456-471},
  keywords={Robots;Collision avoidance;Robot kinematics;Planning;Manipulators;Service robots;Task analysis;Collision avoidance;cooperative robotics;model predictive control (MPC);pick-and-place robotics;real-time trajectory planning},
  doi={10.1109/TCST.2023.3331525}}

@misc{drolet2024comparisonimitationlearningalgorithms,
      title={A Comparison of Imitation Learning Algorithms for Bimanual Manipulation}, 
      author={Michael Drolet and Simon Stepputtis and Siva Kailas and Ajinkya Jain and Jan Peters and Stefan Schaal and Heni Ben Amor},
      year={2024},
      eprint={2408.06536},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2408.06536}, 
}

@misc{andrychowicz2018hindsightexperiencereplay,
      title={Hindsight Experience Replay}, 
      author={Marcin Andrychowicz and Filip Wolski and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
      year={2018},
      eprint={1707.01495},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.01495}, 
}

@misc{luo2024serlsoftwaresuitesampleefficient,
      title={SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning}, 
      author={Jianlan Luo and Zheyuan Hu and Charles Xu and You Liang Tan and Jacob Berg and Archit Sharma and Stefan Schaal and Chelsea Finn and Abhishek Gupta and Sergey Levine},
      year={2024},
      eprint={2401.16013},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2401.16013}, 
}

@misc{ball2023efficientonlinereinforcementlearning,
      title={Efficient Online Reinforcement Learning with Offline Data}, 
      author={Philip J. Ball and Laura Smith and Ilya Kostrikov and Sergey Levine},
      year={2023},
      eprint={2302.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.02948}, 
}

@article{doi:10.1177/0278364918765952,
author = {Seyed Sina Mirrazavi Salehian and Nadia Figueroa and Aude Billard},
title ={A unified framework for coordinated multi-arm motion planning},
journal = {The International Journal of Robotics Research},
volume = {37},
number = {10},
pages = {1205-1232},
year = {2018},
doi = {10.1177/0278364918765952},
URL = {https://doi.org/10.1177/0278364918765952},
eprint = {https://doi.org/10.1177/0278364918765952},
}

@misc{luo2024precisedexterousroboticmanipulation,
      title={Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning}, 
      author={Jianlan Luo and Charles Xu and Jeffrey Wu and Sergey Levine},
      year={2024},
      eprint={2410.21845},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.21845}, 
}

@misc{kataoka2022bimanualmanipulationattachmentsimtoreal,
      title={Bi-Manual Manipulation and Attachment via Sim-to-Real Reinforcement Learning}, 
      author={Satoshi Kataoka and Seyed Kamyar Seyed Ghasemipour and Daniel Freeman and Igor Mordatch},
      year={2022},
      eprint={2203.08277},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2203.08277}, 
}

@misc{lin2023bitouchbimanualtactilemanipulation,
      title={Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning}, 
      author={Yijiong Lin and Alex Church and Max Yang and Haoran Li and John Lloyd and Dandan Zhang and Nathan F. Lepora},
      year={2023},
      eprint={2307.06423},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2307.06423}, 
}

@article{KRUGER20115,
title = {Dual arm robot for flexible and cooperative assembly},
journal = {CIRP Annals},
volume = {60},
number = {1},
pages = {5-8},
year = {2011},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2011.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0007850611000187},
}

@misc{zhao2023learningfinegrainedbimanualmanipulation,
      title={Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware}, 
      author={Tony Z. Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn},
      year={2023},
      eprint={2304.13705},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2304.13705}, 
}

@misc{song2023hybridrlusingoffline,
      title={Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient}, 
      author={Yuda Song and Yifei Zhou and Ayush Sekhari and J. Andrew Bagnell and Akshay Krishnamurthy and Wen Sun},
      year={2023},
      eprint={2210.06718},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.06718}, 
}

@misc{kostrikov2021imageaugmentationneedregularizing,
      title={Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels}, 
      author={Ilya Kostrikov and Denis Yarats and Rob Fergus},
      year={2021},
      eprint={2004.13649},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.13649}, 
}

@misc{liu2022goalconditionedreinforcementlearningproblems,
      title={Goal-Conditioned Reinforcement Learning: Problems and Solutions}, 
      author={Minghuan Liu and Menghui Zhu and Weinan Zhang},
      year={2022},
      eprint={2201.08299},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2201.08299}, 
}

@misc{jin2018qlearningprovablyefficient,
      title={Is Q-learning Provably Efficient?}, 
      author={Chi Jin and Zeyuan Allen-Zhu and Sebastien Bubeck and Michael I. Jordan},
      year={2018},
      eprint={1807.03765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.03765}, 
}

@misc{jin2019provablyefficientreinforcementlearning,
      title={Provably Efficient Reinforcement Learning with Linear Function Approximation}, 
      author={Chi Jin and Zhuoran Yang and Zhaoran Wang and Michael I. Jordan},
      year={2019},
      eprint={1907.05388},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.05388}, 
}

@misc{azar2012samplecomplexityreinforcementlearning,
      title={On the Sample Complexity of Reinforcement Learning with a Generative Model}, 
      author={Mohammad Gheshlaghi Azar and Remi Munos and Bert Kappen},
      year={2012},
      eprint={1206.6461},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1206.6461}, 
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@misc{nair2018overcomingexplorationreinforcementlearning,
      title={Overcoming Exploration in Reinforcement Learning with Demonstrations}, 
      author={Ashvin Nair and Bob McGrew and Marcin Andrychowicz and Wojciech Zaremba and Pieter Abbeel},
      year={2018},
      eprint={1709.10089},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1709.10089}, 
}

@misc{sutter2025comparisonvisualrepresentationsrealworld,
      title={A comparison of visual representations for real-world reinforcement learning in the context of vacuum gripping}, 
      author={Nico Sutter and Valentin N. Hartmann and Stelian Coros},
      year={2025},
      eprint={2503.02405},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2503.02405}, 
}

@misc{ankile2024imitationrefinementresidual,
      title={From Imitation to Refinement -- Residual RL for Precise Assembly}, 
      author={Lars Ankile and Anthony Simeonov and Idan Shenfeld and Marcel Torne and Pulkit Agrawal},
      year={2024},
      eprint={2407.16677},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.16677}, 
}

@article{cbf00562-4ec1-30aa-96c9-a7b516c45b2b,
 ISSN = {00959057, 19435274},
 URL = {http://www.jstor.org/stable/24900506},
 author = {RICHARD BELLMAN},
 journal = {Journal of Mathematics and Mechanics},
 number = {5},
 pages = {679--684},
 publisher = {Indiana University Mathematics Department},
 title = {A Markovian Decision Process},
 urldate = {2025-04-02},
 volume = {6},
 year = {1957}
}

@article{KAELBLING199899,
title = {Planning and acting in partially observable stochastic domains},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {99-134},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00023-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
keywords = {Planning, Uncertainty, Partially observable Markov decision processes},
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
}

@inproceedings{10.5555/1620270.1620297,
author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
title = {Maximum entropy inverse reinforcement learning},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
pages = {1433–1438},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}

@misc{haarnoja2018softactorcriticoffpolicymaximum,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}

@misc{kressgazit2024robotlearningempiricalscience,
      title={Robot Learning as an Empirical Science: Best Practices for Policy Evaluation}, 
      author={Hadas Kress-Gazit and Kunimatsu Hashimoto and Naveen Kuppuswamy and Paarth Shah and Phoebe Horgan and Gordon Richardson and Siyuan Feng and Benjamin Burchfiel},
      year={2024},
      eprint={2409.09491},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2409.09491}, 
}

@misc{johannink2018residualreinforcementlearningrobot,
      title={Residual Reinforcement Learning for Robot Control}, 
      author={Tobias Johannink and Shikhar Bahl and Ashvin Nair and Jianlan Luo and Avinash Kumar and Matthias Loskyll and Juan Aparicio Ojea and Eugen Solowjow and Sergey Levine},
      year={2018},
      eprint={1812.03201},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/1812.03201}, 
}

@misc{ankile2024imitationrefinementresidual,
        title={From Imitation to Refinement -- Residual RL for Precise Assembly}, 
        author={Lars Ankile and Anthony Simeonov and Idan Shenfeld and Marcel Torne and Pulkit Agrawal},
        year={2024},
        eprint={2407.16677},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2407.16677}, 
  }

@article{GARRIDOJURADO20142280,
title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
journal = {Pattern Recognition},
volume = {47},
number = {6},
pages = {2280-2292},
year = {2014},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2014.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0031320314000235},
author = {S. Garrido-Jurado and R. Muñoz-Salinas and F.J. Madrid-Cuevas and M.J. Marín-Jiménez},
keywords = {Augmented reality, Fiducial marker, Computer vision},
abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.}
}

@misc{huang2024mrhermodelbasedrelayhindsight,
      title={MRHER: Model-based Relay Hindsight Experience Replay for Sequential Object Manipulation Tasks with Sparse Rewards}, 
      author={Yuming Huang and Bin Ren and Ziming Xu and Lianghong Wu},
      year={2024},
      eprint={2306.16061},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2306.16061}, 
}

@misc{makarova2025contherhumanlikecontextualrobot,
      title={CONTHER: Human-Like Contextual Robot Learning via Hindsight Experience Replay and Transformers without Expert Demonstrations}, 
      author={Maria Makarova and Qian Liu and Dzmitry Tsetserukou},
      year={2025},
      eprint={2503.15895},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2503.15895}, 
}

@article{Horv_th_2024,
   title={HiER: Highlight Experience Replay for Boosting Off-Policy Reinforcement Learning Agents},
   volume={12},
   ISSN={2169-3536},
   url={http://dx.doi.org/10.1109/ACCESS.2024.3427012},
   DOI={10.1109/access.2024.3427012},
   journal={IEEE Access},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Horváth, Dániel and Bujalance Martín, Jesús and Gàbor Erdos, Ferenc and Istenes, Zoltán and Moutarde, Fabien},
   year={2024},
   pages={100102–100119} }

@misc{wang2025oneshotdualarmimitationlearning,
      title={One-Shot Dual-Arm Imitation Learning}, 
      author={Yilong Wang and Edward Johns},
      year={2025},
      eprint={2503.06831},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2503.06831}, 
}

@misc{mu2025robotwindualarmrobotbenchmark,
      title={RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)}, 
      author={Yao Mu and Tianxing Chen and Shijia Peng and Zanxin Chen and Zeyu Gao and Yude Zou and Lunkai Lin and Zhiqiang Xie and Ping Luo},
      year={2025},
      eprint={2409.02920},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2409.02920}, 
}

@misc{wang2025learningdualarmcoordinationgrasping,
      title={Learning Dual-Arm Coordination for Grasping Large Flat Objects}, 
      author={Yongliang Wang and Hamidreza Kasaei},
      year={2025},
      eprint={2504.03500},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2504.03500}, 
}

@misc{wang2025maernavbidirectionalmotionlearning,
      title={MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented Experience Replay for Robot Navigation}, 
      author={Shanze Wang and Mingao Tan and Zhibo Yang and Biao Huang and Xiaoyu Shen and Hailong Huang and Wei Zhang},
      year={2025},
      eprint={2503.23908},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2503.23908}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{drolet2024comparisonimitationlearningalgorithms,
      title={A Comparison of Imitation Learning Algorithms for Bimanual Manipulation}, 
      author={Michael Drolet and Simon Stepputtis and Siva Kailas and Ajinkya Jain and Jan Peters and Stefan Schaal and Heni Ben Amor},
      year={2024},
      eprint={2408.06536},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2408.06536}, 
}